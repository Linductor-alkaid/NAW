# 服务层设计方案

> **参考文档**：本设计方案基于 SiliconFlow（硅基流动）官方API文档设计
> - API文档: https://docs.siliconflow.cn/cn/api-reference/chat-completions/chat-completions
> - 模型列表: https://cloud.siliconflow.cn/models
> - API Key: https://cloud.siliconflow.cn/account/ak
> - 快速开始: https://docs.siliconflow.com/quickstart/models
> 
> **重要提示**：SiliconFlow API 完全兼容 OpenAI API 格式，使用 `.cn` 域名（`api.siliconflow.cn`），`.com` 域名已计划停用。

## 1. 概述

### 1.1 设计目标
服务层作为Agent桌宠应用的核心中间层，负责与硅基流动（SiliconFlow）API交互，提供统一的AI服务接口。本方案旨在：
- **模型多样性利用**：充分利用硅基流动平台上的不同模型，针对不同任务类型选择最适合的模型
- **高性能并发**：支持异步请求处理、请求队列管理和并发控制
- **智能上下文管理**：根据任务类型动态构建和管理上下文
- **容错与可靠性**：完善的错误处理、重试机制和降级策略
- **可扩展性**：易于添加新模型、新任务类型和新功能

### 1.2 核心原则
1. **模型专精化**：不同模型处理不同类型任务，发挥各自优势
2. **异步非阻塞**：所有API调用采用异步方式，不阻塞主线程
3. **上下文感知**：根据任务自动选择合适的上下文信息
4. **资源优化**：智能的请求调度和资源管理

## 2. 架构设计

### 2.1 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                        服务层架构                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌────────────────┐  ┌────────────────┐  ┌────────────────┐ │
│  │  任务路由器     │  │  模型管理器     │  │  上下文管理器   │ │
│  │ TaskRouter     │  │ ModelManager   │  │ ContextManager │ │
│  └────────────────┘  └────────────────┘  └────────────────┘ │
│         │                    │                    │         │
│         └────────────────────┼────────────────────┘         │
│                              │                              │
│                     ┌────────────────┐                      │
│                     │  API客户端      │                      │
│                     │ APIClient      │                      │
│                     └────────────────┘                      │
│         ▲                    │                    │         │
│         │                    │                    │         │
│  ┌──────┴─────────┐  ┌───────┴────────┐  ┌────────┴────────┐ │
│  │  语音服务       │  │  视觉服务       │  │  对话触发器      │ │
│  │ SpeechService  │  │ VisionService  │  │ConversationTrigger│ │
│  │ - STT          │  │ - 屏幕采集      │  │ - 响应判断       │ │
│  │ - TTS          │  │ - VLM调用      │  │ - 主动触发       │ │
│  └────────────────┘  └────────────────┘  └─────────────────┘ │
│         │                    │                    │         │
│         └────────────────────┼────────────────────┘         │
│                              │                              │
│         ┌────────────────────┼────────────────────┐         │
│         │                    │                    │         │
│  ┌────────────────┐  ┌────────────────┐  ┌────────────────┐ │
│  │  请求管理器     │  │  响应处理器     │  │  缓存管理器     │ │
│  │ RequestManager │  │ResponseHandler │  │  CacheManager  │ │
│  └────────────────┘  └────────────────┘  └────────────────┘ │
│                                                             │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
                    SiliconFlow API
                    (LLM / VLM / STT / TTS)
```

### 2.2 核心模块

#### 2.2.1 任务路由器 (TaskRouter)
- **职责**：根据任务类型选择最合适的模型
- **输入**：任务类型、任务描述、优先级
- **输出**：选定的模型配置和任务执行计划

#### 2.2.2 模型管理器 (ModelManager)
- **职责**：管理所有可用模型及其配置
- **功能**：
  - 模型注册和配置
  - 模型能力映射（模型适合哪些任务）
  - 模型健康状态监控
  - 模型性能统计

#### 2.2.3 上下文管理器 (ContextManager)
- **职责**：根据任务类型构建合适的上下文
- **功能**：
  - 对话历史管理
  - 项目上下文收集（通过MCP）
  - Agent状态上下文
  - 上下文窗口管理（Token限制）

#### 2.2.4 API客户端 (APIClient)
- **职责**：封装硅基流动API调用
- **功能**：
  - HTTP请求封装
  - 认证管理
  - 请求/响应序列化
  - 基础错误处理

#### 2.2.5 请求管理器 (RequestManager)
- **职责**：管理请求队列和并发控制
- **功能**：
  - 请求队列管理
  - 并发限制（防止API限流）
  - 请求优先级调度
  - 超时管理

#### 2.2.6 响应处理器 (ResponseHandler)
- **职责**：处理API响应
- **功能**：
  - 流式响应处理
  - JSON解析和验证
  - 错误处理和重试
  - 响应缓存

#### 2.2.7 缓存管理器 (CacheManager)
- **职责**：缓存常见请求的响应
- **功能**：
  - 基于请求内容的缓存键生成
  - 缓存过期策略
  - 缓存统计

#### 2.2.8 语音服务 (SpeechService)
- **职责**：处理语音输入输出
- **功能**：
  - STT (Speech-to-Text)：语音转文本
  - TTS (Text-to-Speech)：文本转语音
  - 音频流处理
  - 语音活动检测 (VAD)

#### 2.2.9 视觉服务 (VisionService)
- **职责**：处理视觉信息
- **功能**：
  - 屏幕截图和采集
  - 图像预处理
  - VLM (Vision Language Model) 调用
  - 屏幕信息理解

#### 2.2.10 对话触发器 (ConversationTrigger)
- **职责**：智能判断何时响应和主动对话
- **功能**：
  - 语音输入判断（是否需要响应）
  - 主动对话触发
  - 使用免费VLM快速判断场景
  - 对话优先级管理

#### 2.2.11 工具管理器 (ToolManager)
- **职责**：管理所有可用工具，支持 Function Calling
- **功能**：
  - 工具注册和管理
  - Function Calling 协议支持
  - 工具执行和结果处理
  - 工具权限和安全控制

#### 2.2.12 MCP服务 (MCPService)
- **职责**：实现 MCP (Model Context Protocol) 协议
- **功能**：
  - MCP Server 实现
  - 代码工具提供（read_file, list_files, search_code等）
  - 项目上下文收集
  - MCP 协议消息处理

## 3. 模型配置与任务映射

### 3.1 任务类型定义

```cpp
enum class TaskType {
    // 对话类任务
    CasualChat,          // 日常对话
    CodeDiscussion,      // 代码讨论
    TechnicalQnA,        // 技术问答
    
    // 代码相关任务
    CodeGeneration,      // 代码生成
    CodeAnalysis,        // 代码分析
    CodeReview,          // 代码审查
    CodeExplanation,     // 代码解释
    BugFix,              // Bug修复
    
    // 项目理解任务
    ProjectAnalysis,     // 项目分析
    ArchitectureDesign,  // 架构设计
    Documentation,       // 文档生成
    
    // Agent相关任务
    AgentDecision,       // Agent决策辅助
    AgentReasoning,      // Agent推理
    ContextUnderstanding, // 上下文理解
    
    // 语音视觉相关任务
    SpeechRecognition,   // 语音识别 (STT)
    SpeechSynthesis,     // 语音合成 (TTS)
    VisionUnderstanding, // 视觉理解 (VLM)
    SceneAnalysis,       // 场景分析（是否需要响应）
    ProactiveResponse,   // 主动响应
    
    // 工具调用相关任务
    ToolCalling,         // 工具调用 (Function Calling)
    CodeToolExecution    // 代码工具执行
};
```

### 3.2 模型配置

> **注意**：以下模型配置为示例，实际使用时应根据 SiliconFlow 平台当前可用的模型列表进行调整。模型ID、上下文长度、价格等信息可能会变化，请在 [SiliconFlow 模型列表](https://cloud.siliconflow.cn/models) 查询最新信息。

#### 3.2.1 模型能力描述

| 模型ID | 擅长任务 | 特点 | 推荐用途 | 备注 |
|--------|---------|------|---------|------|
| deepseek-ai/DeepSeek-V3 | 代码生成、代码分析、技术问答、复杂推理 | 强大的代码理解和推理能力，长上下文 | 代码相关任务、项目分析、代码审查 | 需查询实际可用版本 |
| deepseek-ai/DeepSeek-Coder-V2 | 代码生成、Bug修复 | 专业代码模型，代码能力突出 | 代码生成、Bug修复、代码补全 | 确认实际模型ID格式 |
| Qwen/Qwen2-72B-Instruct | 通用对话、上下文理解 | 均衡性能，多语言支持 | 日常对话、Agent决策、推理 | Qwen2系列，需确认具体可用版本 |
| 01-ai/Yi-1.5-34B-Chat-16K | 快速响应、简单任务 | 响应速度快，成本低 | 简单对话、快速查询 | 16K上下文长度 |
| Qwen/Qwen2-7B-Instruct | 轻量级对话任务 | 免费模型，速度快 | 简单对话、测试 | 免费模型选项 |
| Qwen/Qwen2-VL-2B-Instruct | 快速视觉理解 | 轻量级VLM，快速响应 | 场景快速判断 | 免费/低成本VLM |
| Qwen/Qwen2-VL-7B-Instruct | 详细视觉理解 | 更强的视觉理解能力 | 详细场景分析、屏幕理解 | 需查询实际可用版本 |

> **重要提示**：
> - 模型ID格式为 `提供方/模型名称`
> - API Base URL: `https://api.siliconflow.cn/v1` (注意使用 .cn 域名)
> - API Key 获取地址: https://cloud.siliconflow.cn/account/ak
> - 完整模型列表: https://cloud.siliconflow.cn/models
> - 实际模型配置参数（上下文长度、价格等）需要查询平台最新信息

#### 3.2.2 模型配置结构

```cpp
struct ModelConfig {
    std::string modelId;              // 模型ID（如 "deepseek-ai/DeepSeek-V2.5"）
    std::string displayName;          // 显示名称
    std::vector<TaskType> supportedTasks; // 支持的任务类型
    uint32_t maxContextTokens;        // 最大上下文Token数
    float defaultTemperature;         // 默认温度参数
    uint32_t defaultMaxTokens;        // 默认最大生成Token数
    float costPer1kTokens;            // 每1K Token的成本（用于成本优化）
    uint32_t maxConcurrentRequests;   // 最大并发请求数
    bool supportsStreaming;           // 是否支持流式响应
    std::string recommendedPromptStyle; // 推荐的提示词风格
    float performanceScore;           // 性能评分（0-1）
};
```

### 3.3 任务到模型映射策略

```cpp
class TaskRouter {
public:
    struct RoutingDecision {
        std::string modelId;
        ModelConfig modelConfig;
        float confidence;             // 选择该模型的置信度（0-1）
        std::string reason;           // 选择原因
    };
    
    RoutingDecision routeTask(
        TaskType taskType,
        const TaskContext& context,
        TaskPriority priority = TaskPriority::Normal
    );
    
private:
    // 优先级映射（任务类型 -> 模型列表，按优先级排序）
    std::unordered_map<TaskType, std::vector<ModelPreference>> m_routingTable;
    
    // 考虑因素：
    // 1. 任务类型与模型能力的匹配度
    // 2. 上下文大小（选择能容纳上下文的模型）
    // 3. 成本（对于非关键任务，选择成本较低的模型）
    // 4. 当前负载（选择负载较低的模型）
    // 5. 性能需求（关键任务选择高性能模型）
};
```

### 3.4 智能模型选择算法

```cpp
class ModelSelector {
public:
    struct SelectionCriteria {
        TaskType taskType;
        size_t contextSize;           // 预估上下文大小（Token）
        TaskPriority priority;        // 任务优先级
        float maxCost;                // 最大成本限制（可选）
        bool requiresStreaming;       // 是否需要流式响应
        std::string preferredModel;   // 偏好模型（可选）
    };
    
    std::string selectBestModel(const SelectionCriteria& criteria);
    
private:
    float calculateScore(
        const ModelConfig& model,
        const SelectionCriteria& criteria
    ) {
        float score = 0.0f;
        
        // 1. 能力匹配度（40%）
        if (model.supportsTask(criteria.taskType)) {
            score += 0.4f;
        }
        
        // 2. 上下文容量（20%）
        if (model.maxContextTokens >= criteria.contextSize) {
            score += 0.2f;
        } else {
            score += 0.2f * (model.maxContextTokens / criteria.contextSize);
        }
        
        // 3. 性能评分（20%）
        score += 0.2f * model.performanceScore;
        
        // 4. 成本效率（10%）
        if (criteria.priority != TaskPriority::Critical) {
            score += 0.1f * (1.0f - std::min(model.costPer1kTokens / 10.0f, 1.0f));
        }
        
        // 5. 负载情况（10%）
        float loadFactor = getModelLoadFactor(model.modelId);
        score += 0.1f * (1.0f - loadFactor);
        
        return score;
    }
};
```

## 4. 上下文管理

### 4.1 上下文类型

```cpp
enum class ContextType {
    ConversationHistory,  // 对话历史
    AgentState,          // Agent状态
    ProjectContext,      // 项目上下文
    CodeContext,         // 代码上下文
    MemoryEvents         // 记忆事件
};
```

### 4.2 上下文构建器

```cpp
class ContextBuilder {
public:
    struct ContextConfig {
        TaskType taskType;
        size_t maxTokens;            // 最大Token数
        bool includeConversationHistory;  // 是否包含对话历史
        bool includeAgentState;      // 是否包含Agent状态
        bool includeProjectContext;  // 是否包含项目上下文
        bool includeMemoryEvents;    // 是否包含记忆事件
        size_t maxHistoryMessages;   // 最大历史消息数
    };
    
    // 构建完整的上下文
    std::vector<ChatMessage> buildContext(
        const ContextConfig& config,
        const std::string& userMessage
    );
    
private:
    // 构建对话历史上下文
    std::vector<ChatMessage> buildConversationHistory(
        size_t maxMessages,
        size_t maxTokens
    );
    
    // 构建Agent状态上下文
    ChatMessage buildAgentStateContext(const naw::agent::Agent& agent);
    
    // 构建项目上下文
    ChatMessage buildProjectContext(
        const std::string& projectPath,
        const TaskType& taskType
    );
    
    // 构建代码上下文
    ChatMessage buildCodeContext(
        const std::vector<std::string>& filePaths,
        const std::string& focusArea
    );
    
    // 构建记忆事件上下文
    ChatMessage buildMemoryContext(
        const std::vector<MemoryEvent>& events,
        const TaskType& taskType
    );
    
    // Token计数
    size_t estimateTokens(const std::string& text);
    size_t estimateTokens(const std::vector<ChatMessage>& messages);
};
```

### 4.3 上下文窗口管理

```cpp
class ContextWindowManager {
public:
    // 智能裁剪上下文，保留最重要的部分
    std::vector<ChatMessage> trimContext(
        const std::vector<ChatMessage>& messages,
        size_t maxTokens,
        TaskType taskType
    );
    
private:
    // 根据任务类型计算消息重要性
    float calculateMessageImportance(
        const ChatMessage& message,
        TaskType taskType
    );
    
    // 保留策略：
    // 1. System prompt始终保留
    // 2. 最近的用户消息优先保留
    // 3. 根据任务类型保留相关历史
    // 4. 重要事件（如关键决策）优先保留
};
```

## 5. 工具调用与MCP服务设计

### 5.1 Function Calling (工具调用) 支持

#### 5.1.1 概述
SiliconFlow API 支持 OpenAI 兼容的 Function Calling，允许 LLM 调用外部工具。这对于代码开发辅助功能至关重要，使得 Agent 可以读取文件、搜索代码、分析项目结构等。

#### 5.1.2 工具定义

```cpp
struct Tool {
    std::string name;
    std::string description;
    nlohmann::json parameters;  // JSON Schema 格式的参数定义
    std::function<nlohmann::json(const nlohmann::json&)> handler;
};

class ToolManager {
public:
    // 注册工具
    void registerTool(const Tool& tool);
    
    // 获取所有工具的定义（用于 Function Calling）
    std::vector<nlohmann::json> getToolsForAPI() const;
    
    // 执行工具调用
    nlohmann::json executeTool(
        const std::string& toolName,
        const nlohmann::json& arguments
    );
    
    // 批量执行工具调用
    std::vector<nlohmann::json> executeTools(
        const std::vector<nlohmann::json>& toolCalls
    );
    
private:
    std::unordered_map<std::string, Tool> m_tools;
};
```

#### 5.1.3 代码工具定义

```cpp
class CodeTools {
public:
    static void registerAllTools(ToolManager& toolManager) {
        // 读取文件工具
        toolManager.registerTool({
            "read_file",
            "读取代码文件的内容。用于查看源代码、配置文件等。",
            {
                {"type", "object"},
                {"properties", {
                    {"path", {
                        {"type", "string"},
                        {"description", "文件路径，相对于项目根目录"}
                    }},
                    {"start_line", {
                        {"type", "integer"},
                        {"description", "起始行号（可选，用于部分读取）"}
                    }},
                    {"end_line", {
                        {"type", "integer"},
                        {"description", "结束行号（可选）"}
                    }}
                }},
                {"required", {"path"}}
            },
            [](const nlohmann::json& args) -> nlohmann::json {
                std::string path = args["path"];
                int startLine = args.value("start_line", -1);
                int endLine = args.value("end_line", -1);
                
                std::string content = readFile(path);
                if (startLine > 0 && endLine > startLine) {
                    content = extractLines(content, startLine, endLine);
                }
                
                return {
                    {"content", content},
                    {"path", path},
                    {"line_count", countLines(content)}
                };
            }
        });
        
        // 列出文件工具
        toolManager.registerTool({
            "list_files",
            "列出项目目录中的文件。可以指定目录路径和文件类型过滤。",
            {
                {"type", "object"},
                {"properties", {
                    {"directory", {
                        {"type", "string"},
                        {"description", "目录路径，相对于项目根目录，默认为根目录"}
                    }},
                    {"pattern", {
                        {"type", "string"},
                        {"description", "文件匹配模式（如 '*.cpp', '*.h'），可选"}
                    }},
                    {"recursive", {
                        {"type", "boolean"},
                        {"description", "是否递归列出子目录，默认false"}
                    }}
                }},
                {"required", {}}
            },
            [](const nlohmann::json& args) -> nlohmann::json {
                std::string dir = args.value("directory", ".");
                std::string pattern = args.value("pattern", "");
                bool recursive = args.value("recursive", false);
                
                auto files = listFiles(dir, pattern, recursive);
                
                return {
                    {"files", files},
                    {"count", files.size()}
                };
            }
        });
        
        // 搜索代码工具
        toolManager.registerTool({
            "search_code",
            "在项目中搜索代码。支持文本搜索、正则表达式搜索等。",
            {
                {"type", "object"},
                {"properties", {
                    {"query", {
                        {"type", "string"},
                        {"description", "搜索查询文本或正则表达式"}
                    }},
                    {"directory", {
                        {"type", "string"},
                        {"description", "搜索目录，默认为项目根目录"}
                    }},
                    {"file_pattern", {
                        {"type", "string"},
                        {"description", "文件类型过滤（如 '*.cpp', '*.h'）"}
                    }},
                    {"case_sensitive", {
                        {"type", "boolean"},
                        {"description", "是否区分大小写，默认false"}
                    }}
                }},
                {"required", {"query"}}
            },
            [](const nlohmann::json& args) -> nlohmann::json {
                std::string query = args["query"];
                std::string dir = args.value("directory", ".");
                std::string pattern = args.value("file_pattern", "");
                bool caseSensitive = args.value("case_sensitive", false);
                
                auto results = searchInFiles(query, dir, pattern, caseSensitive);
                
                return {
                    {"matches", results},
                    {"total_matches", results.size()}
                };
            }
        });
        
        // 获取项目结构工具
        toolManager.registerTool({
            "get_project_structure",
            "获取项目的整体结构信息，包括目录结构、CMake配置、依赖关系等。",
            {
                {"type", "object"},
                {"properties", {
                    {"include_files", {
                        {"type", "boolean"},
                        {"description", "是否包含文件列表，默认true"}
                    }},
                    {"include_dependencies", {
                        {"type", "boolean"},
                        {"description", "是否包含依赖关系，默认true"}
                    }}
                }},
                {"required", {}}
            },
            [](const nlohmann::json& args) -> nlohmann::json {
                bool includeFiles = args.value("include_files", true);
                bool includeDeps = args.value("include_dependencies", true);
                
                auto structure = analyzeProjectStructure(includeFiles, includeDeps);
                
                return structure;
            }
        });
        
        // 分析代码工具
        toolManager.registerTool({
            "analyze_code",
            "分析代码文件，提取函数、类、依赖关系等信息。",
            {
                {"type", "object"},
                {"properties", {
                    {"path", {
                        {"type", "string"},
                        {"description", "代码文件路径"}
                    }},
                    {"analysis_type", {
                        {"type", "string"},
                        {"enum", {"functions", "classes", "dependencies", "all"}},
                        {"description", "分析类型，默认'all'"}
                    }}
                }},
                {"required", {"path"}}
            },
            [](const nlohmann::json& args) -> nlohmann::json {
                std::string path = args["path"];
                std::string type = args.value("analysis_type", "all");
                
                auto analysis = analyzeCodeFile(path, type);
                
                return analysis;
            }
        });
        
        // 更多工具...
    }
};
```

#### 5.1.4 Function Calling 流程

```cpp
class FunctionCallingHandler {
public:
    struct FunctionCallResult {
        std::string toolCallId;
        std::string toolName;
        nlohmann::json result;
        std::string error;  // 如果执行失败
    };
    
    // 处理LLM返回的function calling请求
    std::vector<FunctionCallResult> handleFunctionCalls(
        const ChatResponse& response,
        ToolManager& toolManager
    ) {
        std::vector<FunctionCallResult> results;
        
        // 检查响应中是否包含工具调用
        if (response.functionCalls.empty()) {
            return results;
        }
        
        // 执行每个工具调用
        for (const auto& toolCall : response.functionCalls) {
            FunctionCallResult result;
            result.toolCallId = toolCall.id;
            result.toolName = toolCall.name;
            
            try {
                result.result = toolManager.executeTool(
                    toolCall.name,
                    toolCall.arguments
                );
            } catch (const std::exception& e) {
                result.error = e.what();
            }
            
            results.push_back(result);
        }
        
        return results;
    }
    
    // 构建包含工具调用结果的后续请求
    std::vector<ChatMessage> buildFollowUpMessages(
        const std::vector<FunctionCallResult>& results
    ) {
        std::vector<ChatMessage> messages;
        
        for (const auto& result : results) {
            nlohmann::json message = {
                {"role", "tool"},
                {"name", result.toolName},
                {"tool_call_id", result.toolCallId}
            };
            
            if (!result.error.empty()) {
                message["content"] = "错误: " + result.error;
            } else {
                message["content"] = result.result.dump();
            }
            
            messages.push_back(ChatMessage::fromJson(message));
        }
        
        return messages;
    }
};
```

### 5.2 MCP (Model Context Protocol) 服务

#### 5.2.1 MCP 协议概述
MCP 是一个开放标准，用于将语言模型与外部工具和数据源集成。服务层需要实现 MCP Server，提供代码开发相关的工具和上下文。

#### 5.2.2 MCP Server 实现

```cpp
class MCPServer {
public:
    struct MCPMessage {
        std::string jsonrpc = "2.0";
        std::string id;
        std::string method;
        nlohmann::json params;
    };
    
    struct MCPTool {
        std::string name;
        std::string description;
        nlohmann::json inputSchema;
        std::function<nlohmann::json(const nlohmann::json&)> handler;
    };
    
    // 初始化MCP Server
    void initialize(const std::string& projectRoot);
    
    // 处理MCP请求
    nlohmann::json handleRequest(const MCPMessage& request);
    
    // 注册MCP工具
    void registerTool(const MCPTool& tool);
    
    // 列出所有可用工具
    nlohmann::json listTools();
    
    // 调用工具
    nlohmann::json callTool(
        const std::string& toolName,
        const nlohmann::json& arguments
    );
    
private:
    std::vector<MCPTool> m_tools;
    std::string m_projectRoot;
    std::shared_ptr<ToolManager> m_toolManager;
    
    void initializeStandardTools();
};
```

#### 5.2.3 MCP 标准工具实现

```cpp
void MCPServer::initializeStandardTools() {
    // 使用 CodeTools 注册所有代码相关工具
    CodeTools::registerAllTools(*m_toolManager);
    
    // 转换为MCP工具格式
    auto apiTools = m_toolManager->getToolsForAPI();
    for (const auto& tool : apiTools) {
        MCPTool mcpTool;
        mcpTool.name = tool["function"]["name"];
        mcpTool.description = tool["function"]["description"];
        mcpTool.inputSchema = tool["function"]["parameters"];
        
        // 设置处理器
        mcpTool.handler = [this, name = mcpTool.name](
            const nlohmann::json& args
        ) -> nlohmann::json {
            return m_toolManager->executeTool(name, args);
        };
        
        registerTool(mcpTool);
    }
}

nlohmann::json MCPServer::listTools() {
    nlohmann::json result;
    result["jsonrpc"] = "2.0";
    result["id"] = generateId();
    
    nlohmann::json tools = nlohmann::json::array();
    for (const auto& tool : m_tools) {
        tools.push_back({
            {"name", tool.name},
            {"description", tool.description},
            {"inputSchema", tool.inputSchema}
        });
    }
    
    result["result"] = {
        {"tools", tools}
    };
    
    return result;
}

nlohmann::json MCPServer::callTool(
    const std::string& toolName,
    const nlohmann::json& arguments
) {
    auto it = std::find_if(
        m_tools.begin(),
        m_tools.end(),
        [&toolName](const MCPTool& tool) {
            return tool.name == toolName;
        }
    );
    
    if (it == m_tools.end()) {
        throw std::runtime_error("Tool not found: " + toolName);
    }
    
    nlohmann::json result;
    result["jsonrpc"] = "2.0";
    result["id"] = generateId();
    
    try {
        auto toolResult = it->handler(arguments);
        result["result"] = {
            {"content", {
                {"text", toolResult.dump()},
                {"type", "text"}
            }}
        };
    } catch (const std::exception& e) {
        result["error"] = {
            {"code", -32603},
            {"message", "Internal error: " + std::string(e.what())}
        };
    }
    
    return result;
}
```

#### 5.2.4 MCP 与 LLM 集成

```cpp
class MCPIntegration {
public:
    // 将MCP工具转换为Function Calling格式
    std::vector<nlohmann::json> convertMCPToolsToFunctions(
        const std::vector<MCPTool>& mcpTools
    ) {
        std::vector<nlohmann::json> functions;
        
        for (const auto& tool : mcpTools) {
            nlohmann::json function = {
                {"type", "function"},
                {"function", {
                    {"name", tool.name},
                    {"description", tool.description},
                    {"parameters", tool.inputSchema}
                }}
            };
            functions.push_back(function);
        }
        
        return functions;
    }
    
    // 在API请求中包含MCP工具
    ChatRequest buildRequestWithTools(
        const std::vector<ChatMessage>& messages,
        const std::vector<MCPTool>& availableTools
    ) {
        ChatRequest request;
        request.messages = messages;
        request.tools = convertMCPToolsToFunctions(availableTools);
        request.toolChoice = "auto";  // 让模型决定是否调用工具
        
        return request;
    }
};
```

### 5.3 项目上下文收集器

```cpp
class ProjectContextCollector {
public:
    struct ProjectInfo {
        std::string name;
        std::string rootPath;
        std::vector<std::string> sourceFiles;
        std::vector<std::string> headerFiles;
        nlohmann::json cmakeConfig;
        std::vector<std::string> dependencies;
        std::unordered_map<std::string, std::string> fileContents;
    };
    
    // 分析项目结构
    ProjectInfo analyzeProject(const std::string& projectRoot);
    
    // 获取文件的上下文（包含相关文件）
    std::string getFileContext(
        const std::string& filePath,
        const ProjectInfo& projectInfo
    );
    
    // 获取项目摘要
    std::string getProjectSummary(const ProjectInfo& projectInfo);
    
private:
    // 解析CMakeLists.txt
    nlohmann::json parseCMakeLists(const std::string& cmakePath);
    
    // 查找相关文件（通过include等）
    std::vector<std::string> findRelatedFiles(
        const std::string& filePath,
        const ProjectInfo& projectInfo
    );
    
    // 提取依赖关系
    std::vector<std::string> extractDependencies(
        const nlohmann::json& cmakeConfig
    );
};
```

## 6. API客户端设计

### 6.1 API客户端接口

```cpp
class SiliconFlowAPIClient {
public:
    struct ChatRequest {
        std::string model;
        std::vector<ChatMessage> messages;
        float temperature;
        uint32_t maxTokens;
        bool stream;
        std::string stopSequence;    // 停止序列（可选）
        float topP;                  // Top-p采样（可选）
        uint32_t topK;               // Top-k采样（可选）
        
        // Function Calling 相关
        std::vector<nlohmann::json> tools;  // 可用工具列表
        std::string toolChoice;             // "auto", "none", 或具体工具名
    };
    
    struct ToolCall {
        std::string id;
        std::string type;
        FunctionCall function;
    };
    
    struct FunctionCall {
        std::string name;
        nlohmann::json arguments;
    };
    
    struct ChatResponse {
        std::string content;
        std::vector<ToolCall> toolCalls;    // 工具调用请求
        std::string finishReason;           // "stop", "tool_calls", "length"等
        uint32_t promptTokens;
        uint32_t completionTokens;
        uint32_t totalTokens;
    };
    
    
    // 同步调用
    ChatResponse chat(const ChatRequest& request);
    
    // 异步调用
    std::future<ChatResponse> chatAsync(const ChatRequest& request);
    
    // 流式调用
    void chatStream(
        const ChatRequest& request,
        std::function<void(const std::string&)> onChunk,
        std::function<void(const ChatResponse&)> onComplete,
        std::function<void(const std::string&)> onError
    );
    
private:
    std::string m_apiKey;
    std::string m_baseUrl;
    std::shared_ptr<HttpClient> m_httpClient;
    std::shared_ptr<RequestManager> m_requestManager;
};
```

### 5.2 HTTP客户端封装

```cpp
class HttpClient {
public:
    struct HttpRequest {
        std::string method;
        std::string url;
        std::unordered_map<std::string, std::string> headers;
        std::string body;
        uint32_t timeoutMs;
    };
    
    struct HttpResponse {
        int statusCode;
        std::unordered_map<std::string, std::string> headers;
        std::string body;
    };
    
    HttpResponse execute(const HttpRequest& request);
    std::future<HttpResponse> executeAsync(const HttpRequest& request);
    
private:
    // 使用 cpp-httplib 或 libcurl 实现
};
```

## 6. 请求管理

### 6.1 请求队列

```cpp
class RequestQueue {
public:
    struct QueuedRequest {
        std::string requestId;
        TaskType taskType;
        std::string modelId;
        ChatRequest request;
        TaskPriority priority;
        std::chrono::system_clock::time_point enqueueTime;
        std::promise<ChatResponse> promise;
    };
    
    void enqueue(QueuedRequest request);
    QueuedRequest dequeue();
    size_t size() const;
    void clear();
    
private:
    std::priority_queue<QueuedRequest> m_queue;
    std::mutex m_mutex;
};
```

### 6.2 并发控制

```cpp
class ConcurrencyController {
public:
    // 为每个模型维护独立的并发限制
    bool canMakeRequest(const std::string& modelId);
    void registerRequest(const std::string& modelId);
    void releaseRequest(const std::string& modelId);
    
private:
    // 模型ID -> 当前并发数 / 最大并发数
    std::unordered_map<std::string, std::pair<std::atomic<uint32_t>, uint32_t>> m_concurrencyLimits;
    std::mutex m_mutex;
};
```

### 6.3 请求管理器

```cpp
class RequestManager {
public:
    RequestManager(
        std::shared_ptr<SiliconFlowAPIClient> client,
        std::shared_ptr<ConcurrencyController> concurrencyController
    );
    
    // 提交请求（自动排队和调度）
    std::future<ChatResponse> submitRequest(
        const ChatRequest& request,
        TaskType taskType,
        TaskPriority priority = TaskPriority::Normal
    );
    
    // 取消请求
    bool cancelRequest(const std::string& requestId);
    
    // 获取统计信息
    struct Statistics {
        uint64_t totalRequests;
        uint64_t completedRequests;
        uint64_t failedRequests;
        float averageResponseTime;
        std::unordered_map<std::string, uint64_t> requestsPerModel;
    };
    Statistics getStatistics() const;
    
private:
    void processQueue();
    std::thread m_workerThread;
    std::atomic<bool> m_running;
};
```

## 7. 响应处理

### 7.1 流式响应处理

```cpp
class StreamResponseHandler {
public:
    void handleStreamChunk(
        const std::string& chunk,
        std::function<void(const std::string&)> onText,
        std::function<void(const ChatResponse&)> onComplete,
        std::function<void(const std::string&)> onError
    );
    
private:
    std::string m_buffer;
    std::string m_accumulatedContent;
    
    // 解析SSE格式的流式数据
    void parseSSEChunk(const std::string& chunk);
};
```

### 7.2 错误处理与重试

```cpp
class ErrorHandler {
public:
    enum class ErrorType {
        NetworkError,       // 网络错误（可重试）
        RateLimitError,     // 限流错误（需延迟重试）
        InvalidRequest,     // 请求错误（不可重试）
        ServerError,        // 服务器错误（可重试）
        TimeoutError,       // 超时错误（可重试）
        UnknownError        // 未知错误
    };
    
    struct RetryPolicy {
        uint32_t maxRetries;
        uint32_t initialDelayMs;
        float backoffMultiplier;  // 退避 multiplier（如 2.0 表示每次延迟翻倍）
        std::unordered_map<ErrorType, bool> retryableErrors;
    };
    
    bool shouldRetry(ErrorType errorType, uint32_t attemptCount);
    uint32_t getRetryDelay(uint32_t attemptCount);
    
private:
    RetryPolicy m_retryPolicy;
};
```

## 8. 缓存管理

### 8.1 缓存策略

```cpp
class CacheManager {
public:
    struct CacheKey {
        std::string modelId;
        std::string messagesHash;    // 消息内容的哈希
        float temperature;
        uint32_t maxTokens;
    };
    
    // 查找缓存
    std::optional<ChatResponse> get(const CacheKey& key);
    
    // 存储缓存
    void put(const CacheKey& key, const ChatResponse& response);
    
    // 清除过期缓存
    void evictExpired();
    
private:
    struct CacheEntry {
        ChatResponse response;
        std::chrono::system_clock::time_point timestamp;
        std::chrono::seconds ttl;  // Time to live
    };
    
    std::unordered_map<CacheKey, CacheEntry> m_cache;
    std::mutex m_mutex;
    
    // 缓存策略：
    // 1. 只缓存非流式响应
    // 2. temperature=0的确定性请求优先缓存
    // 3. 根据任务类型设置不同的TTL
    // 4. 定期清理过期缓存
};
```

## 9. 服务层统一接口

### 9.1 服务层主接口

```cpp
class AIService {
public:
    AIService(
        const std::string& apiKey,
        const std::string& configPath = "config/ai_service_config.json"
    );
    
    // 通用请求接口
    struct ServiceRequest {
        TaskType taskType;
        std::string userMessage;
        TaskPriority priority;
        bool stream;
        std::optional<std::string> preferredModel;
        std::optional<ContextConfig> contextConfig;
    };
    
    // 同步调用
    ChatResponse process(const ServiceRequest& request);
    
    // 异步调用
    std::future<ChatResponse> processAsync(const ServiceRequest& request);
    
    // 流式调用
    void processStream(
        const ServiceRequest& request,
        std::function<void(const std::string&)> onChunk,
        std::function<void(const ChatResponse&)> onComplete,
        std::function<void(const std::string&)> onError
    );
    
    // 便捷方法：对话
    ChatResponse chat(const std::string& message, bool includeAgentState = true);
    
    // 便捷方法：代码生成
    ChatResponse generateCode(
        const std::string& description,
        const std::vector<std::string>& contextFiles = {}
    );
    
    // 便捷方法：代码分析
    ChatResponse analyzeCode(
        const std::string& code,
        const std::string& focus = "general"
    );
    
    // 便捷方法：Agent决策辅助
    ChatResponse assistDecision(
        const std::string& situation,
        const naw::agent::Agent& agent
    );
    
    // 便捷方法：带工具调用的对话
    ChatResponse chatWithTools(
        const std::string& message,
        const std::vector<nlohmann::json>& tools,
        bool includeAgentState = true
    );
    
private:
    std::unique_ptr<TaskRouter> m_taskRouter;
    std::unique_ptr<ModelManager> m_modelManager;
    std::unique_ptr<ContextManager> m_contextManager;
    std::unique_ptr<SiliconFlowAPIClient> m_apiClient;
    std::unique_ptr<RequestManager> m_requestManager;
    std::unique_ptr<CacheManager> m_cacheManager;
    std::unique_ptr<ErrorHandler> m_errorHandler;
    std::unique_ptr<ToolManager> m_toolManager;
    std::unique_ptr<MCPServer> m_mcpServer;
    std::unique_ptr<FunctionCallingHandler> m_functionCallingHandler;
};
```

## 10. 配置管理

### 10.1 配置文件结构

```json
{
  "api": {
    "base_url": "https://api.siliconflow.cn/v1",
    "api_key": "${SILICONFLOW_API_KEY}",
    "api_key_source": "从 https://cloud.siliconflow.cn/account/ak 获取",
    "default_timeout_ms": 30000,
    "note": "使用 .cn 域名，.com 域名已计划停用"
  },
  "models": [
    {
      "model_id": "deepseek-ai/DeepSeek-V3",
      "display_name": "DeepSeek V3",
      "supported_tasks": [
        "CodeGeneration",
        "CodeAnalysis",
        "CodeExplanation",
        "CodeReview",
        "ProjectAnalysis",
        "TechnicalQnA",
        "ArchitectureDesign"
      ],
      "max_context_tokens": 64000,
      "default_temperature": 0.7,
      "default_max_tokens": 4096,
      "cost_per_1k_tokens": 0.14,
      "max_concurrent_requests": 10,
      "supports_streaming": true,
      "performance_score": 0.95,
      "note": "需查询平台实际可用版本和参数"
    },
    {
      "model_id": "deepseek-ai/DeepSeek-Coder-V2",
      "display_name": "DeepSeek Coder V2",
      "supported_tasks": [
        "CodeGeneration",
        "CodeAnalysis",
        "BugFix"
      ],
      "max_context_tokens": 128000,
      "default_temperature": 0.2,
      "default_max_tokens": 4096,
      "cost_per_1k_tokens": 0.55,
      "max_concurrent_requests": 5,
      "supports_streaming": true,
      "performance_score": 0.98,
      "note": "专业代码模型，支持128K上下文，需确认实际模型ID格式"
    },
    {
      "model_id": "Qwen/Qwen2-72B-Instruct",
      "display_name": "Qwen 2 72B",
      "supported_tasks": [
        "CasualChat",
        "CodeDiscussion",
        "AgentDecision",
        "AgentReasoning",
        "ContextUnderstanding"
      ],
      "max_context_tokens": 32768,
      "default_temperature": 0.8,
      "default_max_tokens": 2048,
      "cost_per_1k_tokens": 0.6,
      "max_concurrent_requests": 8,
      "supports_streaming": true,
      "performance_score": 0.90,
      "note": "Qwen2系列，需确认具体可用版本和上下文长度"
    },
    {
      "model_id": "01-ai/Yi-1.5-34B-Chat-16K",
      "display_name": "Yi 1.5 34B Chat 16K",
      "supported_tasks": [
        "CasualChat",
        "CodeDiscussion"
      ],
      "max_context_tokens": 16384,
      "default_temperature": 0.7,
      "default_max_tokens": 1024,
      "cost_per_1k_tokens": 0.15,
      "max_concurrent_requests": 15,
      "supports_streaming": true,
      "performance_score": 0.75,
      "note": "16K上下文版本，需确认实际可用模型ID"
    }
  ],
  "model_info_note": "以上模型配置为示例，实际使用时请访问 https://cloud.siliconflow.cn/models 查询最新可用模型、ID格式、上下文长度和价格信息",
  "routing": {
    "default_model_per_task": {
      "CasualChat": "01-ai/Yi-1.5-34B-Chat-16K",
      "CodeGeneration": "deepseek-ai/DeepSeek-Coder-V2",
      "CodeAnalysis": "deepseek-ai/DeepSeek-V3",
      "AgentDecision": "Qwen/Qwen2-72B-Instruct"
    },
    "fallback_model": "Qwen/Qwen2-72B-Instruct"
  },
  "context": {
    "max_history_messages": 50,
    "max_context_tokens": 60000,
    "default_include_agent_state": true,
    "default_include_project_context": true
  },
  "request_management": {
    "max_queue_size": 100,
    "default_timeout_ms": 30000,
    "max_retries": 3,
    "retry_delay_ms": 1000,
    "retry_backoff_multiplier": 2.0
  },
  "cache": {
    "enabled": true,
    "default_ttl_seconds": 3600,
    "max_cache_size": 1000,
    "ttl_per_task_type": {
      "CodeGeneration": 7200,
      "CodeAnalysis": 3600,
      "CasualChat": 1800
    }
  },
  "logging": {
    "level": "info",
    "log_requests": true,
    "log_responses": false,
    "log_errors": true
  },
  "multimodal": {
    "stt": {
      "model_id": "openai/whisper-1",
      "provider": "siliconflow",
      "supported_languages": ["zh-CN", "en-US"],
      "supports_streaming": true,
      "vad_enabled": true,
      "vad_threshold": 0.5
    },
    "tts": {
      "model_id": "openai/tts-1",
      "provider": "siliconflow",
      "supported_voices": ["alloy", "echo", "fable", "onyx", "nova", "shimmer"],
      "default_voice": "nova",
      "supports_streaming": true,
      "default_speed": 1.0,
      "default_pitch": 1.0
    },
    "vlm_fast": {
      "model_id": "Qwen/Qwen2-VL-2B-Instruct",
      "description": "快速场景判断，免费/低成本",
      "max_context_tokens": 8192,
      "supports_streaming": false,
      "use_case": "快速判断是否需要响应"
    },
    "vlm_detailed": {
      "model_id": "Qwen/Qwen2-VL-7B-Instruct",
      "description": "详细场景分析",
      "max_context_tokens": 32768,
      "supports_streaming": false,
      "use_case": "需要详细理解屏幕内容时使用"
    }
  },
  "conversation_trigger": {
    "fast_judge_model": "Qwen/Qwen2-VL-2B-Instruct",
    "screen_capture": {
      "low_frequency_interval_ms": 5000,
      "high_frequency_interval_ms": 1000,
      "max_recent_screens": 10,
      "enable_ocr": false,
      "capture_resolution": "1920x1080"
    },
    "proactive_rules": [
      {
        "name": "error_detection",
        "condition": "screen contains error message",
        "priority": 0.8,
        "enabled": true
      },
      {
        "name": "idle_timeout",
        "condition": "user idle for 5 minutes",
        "priority": 0.5,
        "enabled": true,
        "check_interval_ms": 60000
      }
    ],
    "response_threshold": 0.6,
    "proactive_threshold": 0.7
  },
  "tools": {
    "enabled_tools": [
      "read_file",
      "list_files",
      "search_code",
      "get_project_structure",
      "analyze_code"
    ],
    "project_root": "${PROJECT_ROOT}",
    "max_file_size_mb": 10,
    "allowed_extensions": [".cpp", ".h", ".hpp", ".c", ".cmake", ".txt", ".md", ".json", ".yaml", ".yml"]
  },
  "mcp": {
    "enabled": true,
    "server_port": 8080,
    "project_root": "${PROJECT_ROOT}",
    "available_tools": [
      "read_file",
      "list_files",
      "search_code",
      "get_project_structure",
      "analyze_code",
      "get_file_context",
      "get_project_summary"
    ]
  }
}
```

## 11. 实现细节

### 11.1 文件结构

```
include/naw/desktop_pet/service/
├── AIService.h                 # 服务层主接口
├── TaskRouter.h                # 任务路由器
├── ModelManager.h              # 模型管理器
├── ContextManager.h            # 上下文管理器
├── APIClient.h                 # API客户端
├── RequestManager.h            # 请求管理器
├── ResponseHandler.h           # 响应处理器
├── CacheManager.h              # 缓存管理器
├── ErrorHandler.h              # 错误处理器
├── SpeechService.h             # 语音服务 (STT/TTS)
├── VisionService.h             # 视觉服务 (VLM)
├── ConversationTrigger.h       # 对话触发器
├── ScreenCaptureScheduler.h    # 屏幕采集调度器
├── ToolManager.h               # 工具管理器
├── MCPService.h                # MCP服务
├── FunctionCallingHandler.h    # Function Calling处理器
├── CodeTools.h                 # 代码工具集合
├── ProjectContextCollector.h   # 项目上下文收集器
├── types/
│   ├── TaskType.h              # 任务类型定义
│   ├── ChatMessage.h           # 聊天消息结构
│   ├── ModelConfig.h           # 模型配置结构
│   ├── ScreenInfo.h            # 屏幕信息结构
│   └── VisionAnalysis.h        # 视觉分析结果
└── utils/
    ├── HttpClient.h            # HTTP客户端
    ├── TokenCounter.h          # Token计数器
    └── AudioProcessor.h        # 音频处理器

src/naw/desktop_pet/service/
├── AIService.cpp
├── TaskRouter.cpp
├── ModelManager.cpp
├── ContextManager.cpp
├── APIClient.cpp
├── RequestManager.cpp
├── ResponseHandler.cpp
├── CacheManager.cpp
├── ErrorHandler.cpp
├── SpeechService.cpp
├── VisionService.cpp
├── ConversationTrigger.cpp
├── ScreenCaptureScheduler.cpp
├── ToolManager.cpp
├── MCPService.cpp
├── FunctionCallingHandler.cpp
├── CodeTools.cpp
├── ProjectContextCollector.cpp
└── utils/
    ├── HttpClient.cpp
    ├── TokenCounter.cpp
    └── AudioProcessor.cpp
```

### 11.2 关键实现要点

#### 11.2.1 模型管理初始化

```cpp
void ModelManager::loadConfig(const std::string& configPath) {
    auto config = nlohmann::json::parse(readFile(configPath));
    
    for (const auto& modelJson : config["models"]) {
        ModelConfig model;
        model.modelId = modelJson["model_id"];
        model.displayName = modelJson["display_name"];
        
        for (const auto& taskStr : modelJson["supported_tasks"]) {
            model.supportedTasks.push_back(
                stringToTaskType(taskStr.get<std::string>())
            );
        }
        
        model.maxContextTokens = modelJson["max_context_tokens"];
        model.defaultTemperature = modelJson["default_temperature"];
        // ... 其他字段
        
        m_models[model.modelId] = model;
    }
}
```

#### 11.2.2 智能路由示例

```cpp
TaskRouter::RoutingDecision TaskRouter::routeTask(
    TaskType taskType,
    const TaskContext& context,
    TaskPriority priority
) {
    // 1. 获取支持该任务的所有模型
    auto candidates = m_modelManager->getModelsForTask(taskType);
    
    // 2. 过滤掉无法处理当前上下文的模型
    candidates.erase(
        std::remove_if(candidates.begin(), candidates.end(),
            [&context](const ModelConfig& model) {
                return model.maxContextTokens < context.estimatedTokens;
            }),
        candidates.end()
    );
    
    // 3. 根据优先级和负载选择最佳模型
    ModelConfig* bestModel = nullptr;
    float bestScore = -1.0f;
    
    for (auto& model : candidates) {
        float score = calculateModelScore(model, taskType, priority, context);
        if (score > bestScore) {
            bestScore = score;
            bestModel = &model;
        }
    }
    
    // 4. 返回路由决策
    RoutingDecision decision;
    decision.modelId = bestModel->modelId;
    decision.modelConfig = *bestModel;
    decision.confidence = bestScore;
    decision.reason = generateReason(taskType, *bestModel, priority);
    
    return decision;
}
```

#### 11.2.3 上下文构建示例

```cpp
std::vector<ChatMessage> ContextManager::buildContext(
    const ContextConfig& config,
    const std::string& userMessage
) {
    std::vector<ChatMessage> messages;
    size_t usedTokens = 0;
    
    // 1. 添加System Prompt
    std::string systemPrompt = buildSystemPrompt(config.taskType);
    messages.push_back({"system", systemPrompt});
    usedTokens += estimateTokens(systemPrompt);
    
    // 2. 添加Agent状态（如果需要）
    if (config.includeAgentState && m_agent) {
        auto agentContext = buildAgentStateContext(*m_agent);
        if (usedTokens + estimateTokens(agentContext.content) <= config.maxTokens) {
            messages.push_back(agentContext);
            usedTokens += estimateTokens(agentContext.content);
        }
    }
    
    // 3. 添加项目上下文（如果需要）
    if (config.includeProjectContext && config.taskType == TaskType::CodeGeneration) {
        auto projectContext = buildProjectContext(config.projectPath, config.taskType);
        if (usedTokens + estimateTokens(projectContext.content) <= config.maxTokens) {
            messages.push_back(projectContext);
            usedTokens += estimateTokens(projectContext.content);
        }
    }
    
    // 4. 添加对话历史（裁剪以适配Token限制）
    auto history = buildConversationHistory(
        config.maxHistoryMessages,
        config.maxTokens - usedTokens - estimateTokens(userMessage) - 500  // 预留500 tokens
    );
    messages.insert(messages.end(), history.begin(), history.end());
    
    // 5. 添加当前用户消息
    messages.push_back({"user", userMessage});
    
    return messages;
}
```

## 12. 使用示例

### 12.1 基本使用

```cpp
// 初始化服务
AIService aiService(apiKey, "config/ai_service_config.json");

// 简单对话
auto response = aiService.chat("你好，今天天气怎么样？");

// 代码生成
auto codeResponse = aiService.generateCode(
    "实现一个快速排序算法",
    {"include/sorting.h"}  // 相关文件作为上下文
);

// 代码分析
auto analysisResponse = aiService.analyzeCode(
    readFile("src/main.cpp"),
    "potential_bugs"
);
```

### 12.2 高级使用

```cpp
// 自定义请求
AIService::ServiceRequest request;
request.taskType = TaskType::ArchitectureDesign;
request.userMessage = "设计一个微服务架构";
request.priority = TaskPriority::High;
request.stream = false;

auto response = aiService.process(request);
```

### 12.3 工具调用使用示例

```cpp
// 使用工具调用进行代码分析
auto response = aiService.chatWithTools(
    "请分析 src/main.cpp 文件，找出所有函数的依赖关系",
    mcpServer->getAvailableTools()
);

// 检查响应中是否包含工具调用
if (!response.toolCalls.empty()) {
    // 处理工具调用
    auto toolResults = functionCallingHandler->handleFunctionCalls(
        response,
        toolManager
    );
    
    // 将工具结果发送回LLM获取最终响应
    auto followUpMessages = functionCallingHandler->buildFollowUpMessages(
        toolResults
    );
    
    // 继续对话
    auto finalResponse = aiService.process({
        .taskType = TaskType::CodeAnalysis,
        .messages = followUpMessages
    });
}
```

### 12.4 MCP服务使用示例

```cpp
// 初始化MCP Server
MCPServer mcpServer;
mcpServer.initialize("/path/to/project");

// 获取可用工具
auto tools = mcpServer.listTools();

// 调用MCP工具
auto result = mcpServer.callTool("read_file", {
    {"path", "src/main.cpp"},
    {"start_line", 1},
    {"end_line", 50}
});

// 集成到LLM对话中
auto request = mcpIntegration->buildRequestWithTools(
    messages,
    mcpServer->getAvailableTools()
);
```

// 流式响应
aiService.processStream(
    request,
    [](const std::string& chunk) {
        std::cout << chunk << std::flush;
    },
    [](const ChatResponse& response) {
        std::cout << "\n[完成] Token使用: " << response.totalTokens << std::endl;
    },
    [](const std::string& error) {
        std::cerr << "[错误] " << error << std::endl;
    }
);
```

## 13. 性能优化

### 13.1 并发处理
- 使用线程池处理多个请求
- 每个模型独立的并发限制
- 请求优先级队列

### 13.2 缓存策略
- 对确定性请求（temperature=0）进行缓存
- 基于请求内容哈希的缓存键
- 不同任务类型的差异化TTL

### 13.3 资源管理
- 连接池复用HTTP连接
- 智能的上下文裁剪（保留重要信息）
- 延迟加载非关键配置

## 14. 监控与日志

### 14.1 统计指标
- 请求总数、成功率、失败率
- 各模型的使用情况和性能
- 平均响应时间、Token使用量
- 缓存命中率

### 14.2 日志记录
- 请求/响应日志（可选）
- 错误日志（必需）
- 性能日志
- 模型选择决策日志

## 15. 扩展性考虑

### 15.1 添加新模型
1. 在配置文件中添加模型配置
2. 指定支持的任务类型
3. 系统自动识别和使用

### 15.2 添加新任务类型
1. 在 `TaskType` 枚举中添加新类型
2. 在模型配置中指定支持该任务的模型
3. 可选：在 `ContextManager` 中添加该任务类型的特殊上下文构建逻辑

### 15.3 插件化架构
- 可插拔的模型提供者（未来支持其他AI服务）
- 可插拔的上下文提供者（MCP、文件系统等）
- 可插拔的缓存后端（内存、Redis等）

## 16. SiliconFlow API 参考信息

### 16.1 重要链接
- **API文档**: https://docs.siliconflow.cn/cn/api-reference/chat-completions/chat-completions
- **模型列表**: https://cloud.siliconflow.cn/models
- **API Key获取**: https://cloud.siliconflow.cn/account/ak
- **快速开始**: https://docs.siliconflow.com/quickstart/models

### 16.2 API 特点
- **OpenAI兼容**: SiliconFlow API 完全兼容 OpenAI API 格式
- **Base URL**: `https://api.siliconflow.cn/v1` (注意使用 .cn 域名，.com 已计划停用)
- **认证方式**: Bearer Token，在请求头中使用 `Authorization: Bearer {api_key}`
- **支持功能**:
  - Chat Completions (标准对话)
  - 流式响应 (Streaming)
  - Function Calling
  - JSON 模式
  - FIM (Fill In the Middle) 补全

### 16.3 配置注意事项
1. **模型ID格式**: 使用 `提供方/模型名称` 格式，如 `deepseek-ai/DeepSeek-V3`
2. **实际模型配置**: 模型列表、上下文长度、价格等信息会更新，请在部署前查询最新信息
3. **Rate Limits**: 注意API的速率限制，合理设置并发请求数
4. **成本控制**: 不同模型价格不同，可根据任务类型和优先级选择合适的模型

## 17. 多模态服务设计（STT-VLM-TTS）

### 17.1 概述

为支持日常通用场景的智能对话，服务层需要集成语音和视觉能力：
- **STT (Speech-to-Text)**: 将用户语音转换为文本
- **VLM (Vision Language Model)**: 理解屏幕内容，用于场景判断
- **TTS (Text-to-Speech)**: 将响应文本转换为语音输出

### 17.2 智能对话触发逻辑

#### 17.2.1 设计原则

1. **选择性响应**：不是所有语音输入都需要响应
2. **主动对话**：可以主动发起对话，提供帮助或提醒
3. **智能判断**：使用轻量级VLM快速判断场景
4. **资源优化**：屏幕信息采集采用低频策略（非响应状态）

#### 17.2.2 对话触发流程

```
用户语音输入 (STT)
    ↓
文本 + 屏幕截图（可选，低频）
    ↓
快速场景判断 (免费VLM)
    ↓
    ├─→ 需要响应 → 调用LLM生成响应 → TTS输出
    │
    └─→ 不需要响应 → 忽略/记录日志
    │
主动触发检查（定时/事件驱动）
    ↓
场景分析 (VLM + Agent状态)
    ↓
判断是否需要主动对话
    ↓
生成主动对话内容 → TTS输出
```

### 17.3 语音服务 (SpeechService)

```cpp
class SpeechService {
public:
    struct STTResult {
        std::string text;
        float confidence;
        std::vector<std::string> alternatives;
    };
    
    struct TTSOptions {
        std::string voice;        // 音色选择
        float speed;              // 语速
        float pitch;              // 音调
        std::string format;       // 音频格式 (wav, mp3等)
    };
    
    // STT: 语音转文本
    STTResult speechToText(
        const std::vector<uint8_t>& audioData,
        const std::string& language = "zh-CN"
    );
    
    // 流式STT（实时识别）
    void speechToTextStream(
        std::function<void(const std::vector<uint8_t>&)> audioCallback,
        std::function<void(const std::string&)> textCallback,
        std::function<void(const std::string&)> errorCallback
    );
    
    // TTS: 文本转语音
    std::vector<uint8_t> textToSpeech(
        const std::string& text,
        const TTSOptions& options = {}
    );
    
    // 流式TTS（实时合成）
    void textToSpeechStream(
        const std::string& text,
        const TTSOptions& options,
        std::function<void(const std::vector<uint8_t>&)> audioCallback
    );
    
    // 语音活动检测 (VAD)
    bool detectSpeechActivity(const std::vector<uint8_t>& audioData);
    
private:
    std::shared_ptr<APIClient> m_apiClient;
    std::string m_sttModel;
    std::string m_ttsModel;
};
```

### 17.4 视觉服务 (VisionService)

```cpp
class VisionService {
public:
    struct ScreenInfo {
        std::vector<uint8_t> screenshot;  // 屏幕截图数据
        uint64_t timestamp;
        std::string windowTitle;
        std::string activeApplication;
        std::vector<std::string> visibleText;  // OCR提取的文本（可选）
    };
    
    struct VisionAnalysisResult {
        std::string description;           // 场景描述
        std::string sceneType;             // 场景类型（编码/浏览/聊天等）
        bool requiresResponse;             // 是否需要响应
        float confidence;                  // 置信度
        std::vector<std::string> keywords; // 关键词
    };
    
    // 采集屏幕信息
    ScreenInfo captureScreen(bool includeOCR = false);
    
    // 使用VLM分析场景（快速判断）
    VisionAnalysisResult analyzeSceneQuick(
        const ScreenInfo& screenInfo,
        const std::string& userText = ""
    );
    
    // 使用VLM详细分析
    VisionAnalysisResult analyzeSceneDetailed(
        const ScreenInfo& screenInfo,
        const std::string& userText,
        const std::string& context
    );
    
    // 批量分析（用于主动对话检查）
    std::vector<VisionAnalysisResult> analyzeSceneBatch(
        const std::vector<ScreenInfo>& screens,
        const std::string& context
    );
    
private:
    std::shared_ptr<APIClient> m_apiClient;
    std::string m_vlmModelFast;   // 快速VLM模型（免费/低成本）
    std::string m_vlmModelDetailed; // 详细分析模型
    std::atomic<uint64_t> m_lastCaptureTime;
    uint32_t m_captureIntervalMs; // 采集间隔（低频模式）
};
```

### 17.5 对话触发器 (ConversationTrigger)

```cpp
class ConversationTrigger {
public:
    enum class TriggerReason {
        UserVoice,          // 用户语音输入
        ScreenChange,       // 屏幕变化
        AgentStateChange,   // Agent状态变化
        Scheduled,          // 定时触发
        UserCommand,        // 用户命令
        ProactiveSuggestion // 主动建议
    };
    
    struct TriggerDecision {
        bool shouldRespond;        // 是否应该响应
        TriggerReason reason;      // 触发原因
        float priority;            // 优先级 (0-1)
        std::string context;       // 上下文信息
        std::optional<std::string> suggestedResponse; // 建议响应（可选）
    };
    
    // 判断是否需要响应用户输入
    TriggerDecision shouldRespondToInput(
        const std::string& userText,
        const std::optional<ScreenInfo>& screenInfo = std::nullopt
    );
    
    // 检查是否应该主动对话
    TriggerDecision shouldInitiateConversation(
        const AgentState& agentState,
        const ScreenInfo& currentScreen,
        const std::vector<ScreenInfo>& recentScreens
    );
    
    // 注册主动对话规则
    void registerProactiveRule(
        const std::string& ruleName,
        std::function<bool(const AgentState&, const ScreenInfo&)> condition,
        std::function<std::string(const AgentState&, const ScreenInfo&)> responseGenerator
    );
    
private:
    std::shared_ptr<VisionService> m_visionService;
    std::shared_ptr<ModelManager> m_modelManager;
    
    // 快速判断模型（免费VLM）
    std::string m_fastVLM;
    
    // 主动对话规则
    std::vector<ProactiveRule> m_proactiveRules;
    
    // 判断逻辑
    TriggerDecision quickJudge(
        const std::string& userText,
        const std::optional<ScreenInfo>& screenInfo
    );
};
```

### 17.6 触发判断逻辑实现

```cpp
ConversationTrigger::TriggerDecision 
ConversationTrigger::shouldRespondToInput(
    const std::string& userText,
    const std::optional<ScreenInfo>& screenInfo
) {
    TriggerDecision decision;
    
    // 1. 快速关键词检查（本地，无需API）
    if (isDirectCommand(userText)) {
        decision.shouldRespond = true;
        decision.reason = TriggerReason::UserCommand;
        decision.priority = 1.0f;
        return decision;
    }
    
    // 2. 使用免费VLM快速判断场景
    if (screenInfo.has_value()) {
        auto analysis = m_visionService->analyzeSceneQuick(
            screenInfo.value(),
            userText
        );
        
        decision.shouldRespond = analysis.requiresResponse;
        decision.reason = TriggerReason::UserVoice;
        decision.priority = analysis.confidence;
        decision.context = analysis.description;
        
        // 如果场景明显不需要响应，直接返回
        if (!analysis.requiresResponse && analysis.confidence > 0.8f) {
            return decision;
        }
    }
    
    // 3. 文本内容分析（使用免费/轻量级模型）
    if (userText.length() > 5) {  // 忽略过短的输入
        // 判断是否是疑问句、包含桌宠名称、或明显需要帮助
        if (containsQuestionMarkers(userText) || 
            containsPetName(userText) ||
            containsHelpKeywords(userText)) {
            decision.shouldRespond = true;
            decision.priority = 0.9f;
        }
    }
    
    return decision;
}

ConversationTrigger::TriggerDecision 
ConversationTrigger::shouldInitiateConversation(
    const AgentState& agentState,
    const ScreenInfo& currentScreen,
    const std::vector<ScreenInfo>& recentScreens
) {
    TriggerDecision decision;
    
    // 检查所有主动对话规则
    for (const auto& rule : m_proactiveRules) {
        if (rule.condition(agentState, currentScreen)) {
            decision.shouldRespond = true;
            decision.reason = TriggerReason::ProactiveSuggestion;
            decision.suggestedResponse = rule.responseGenerator(
                agentState, 
                currentScreen
            );
            decision.priority = 0.7f;  // 主动对话优先级稍低
            break;
        }
    }
    
    // 基于屏幕变化的主动对话
    if (!recentScreens.empty()) {
        auto analysis = m_visionService->analyzeSceneQuick(currentScreen);
        if (analysis.sceneType == "error" || 
            analysis.sceneType == "confusion") {
            decision.shouldRespond = true;
            decision.reason = TriggerReason::ScreenChange;
            decision.priority = 0.8f;
        }
    }
    
    return decision;
}
```

### 17.7 屏幕信息采集策略

```cpp
class ScreenCaptureScheduler {
public:
    enum class CaptureMode {
        LowFrequency,    // 低频模式（默认，非响应状态）
        HighFrequency,   // 高频模式（检测到语音输入时）
        OnDemand         // 按需采集（响应判断需要时）
    };
    
    void setMode(CaptureMode mode) {
        m_mode = mode;
        
        switch (mode) {
            case LowFrequency:
                m_intervalMs = 5000;  // 5秒一次
                break;
            case HighFrequency:
                m_intervalMs = 1000;  // 1秒一次
                break;
            case OnDemand:
                m_intervalMs = 0;     // 不自动采集
                break;
        }
    }
    
    // 启动采集循环
    void startCaptureLoop(
        std::function<void(const ScreenInfo&)> onCapture
    ) {
        m_captureThread = std::thread([this, onCapture]() {
            while (m_running) {
                if (m_mode != CaptureMode::OnDemand) {
                    auto screen = m_visionService->captureScreen(false);
                    onCapture(screen);
                }
                std::this_thread::sleep_for(
                    std::chrono::milliseconds(m_intervalMs)
                );
            }
        });
    }
    
private:
    CaptureMode m_mode = CaptureMode::LowFrequency;
    uint32_t m_intervalMs = 5000;
    std::atomic<bool> m_running;
    std::thread m_captureThread;
};
```

### 17.8 完整对话流程

```cpp
class ConversationPipeline {
public:
    void initialize() {
        // 启动低频屏幕采集
        m_screenScheduler.setMode(CaptureMode::LowFrequency);
        m_screenScheduler.startCaptureLoop([this](const ScreenInfo& screen) {
            m_recentScreens.push_back(screen);
            if (m_recentScreens.size() > 10) {
                m_recentScreens.pop_front();
            }
        });
        
        // 启动语音输入监听
        m_speechService->speechToTextStream(
            [this](const std::vector<uint8_t>& audio) {
                // 收到音频
            },
            [this](const std::string& text) {
                handleUserInput(text);
            },
            [this](const std::string& error) {
                // 错误处理
            }
        );
    }
    
private:
    void handleUserInput(const std::string& userText) {
        // 1. 切换到高频屏幕采集
        m_screenScheduler.setMode(CaptureMode::HighFrequency);
        
        // 2. 获取当前屏幕
        auto screen = m_visionService->captureScreen(false);
        
        // 3. 判断是否需要响应
        auto decision = m_trigger->shouldRespondToInput(userText, screen);
        
        if (decision.shouldRespond) {
            // 4. 生成响应
            auto response = m_aiService->chat(userText);
            
            // 5. TTS输出
            auto audio = m_speechService->textToSpeech(response.content);
            playAudio(audio);
        }
        
        // 6. 恢复低频采集
        m_screenScheduler.setMode(CaptureMode::LowFrequency);
    }
    
    void checkProactiveConversation() {
        // 定时检查是否需要主动对话
        auto currentScreen = m_visionService->captureScreen(false);
        auto decision = m_trigger->shouldInitiateConversation(
            m_agentState,
            currentScreen,
            std::vector<ScreenInfo>(m_recentScreens.begin(), m_recentScreens.end())
        );
        
        if (decision.shouldRespond && decision.suggestedResponse) {
            auto audio = m_speechService->textToSpeech(
                decision.suggestedResponse.value()
            );
            playAudio(audio);
        }
    }
    
    std::shared_ptr<SpeechService> m_speechService;
    std::shared_ptr<VisionService> m_visionService;
    std::shared_ptr<ConversationTrigger> m_trigger;
    std::shared_ptr<AIService> m_aiService;
    ScreenCaptureScheduler m_screenScheduler;
    std::deque<ScreenInfo> m_recentScreens;
    AgentState m_agentState;
};
```

### 17.9 模型配置（VLM/STT/TTS）

```json
{
  "multimodal_models": {
    "stt": {
      "model_id": "openai/whisper-1",
      "provider": "siliconflow",
      "supported_languages": ["zh-CN", "en-US"],
      "supports_streaming": true
    },
    "tts": {
      "model_id": "openai/tts-1",
      "provider": "siliconflow",
      "supported_voices": ["alloy", "echo", "fable", "onyx", "nova", "shimmer"],
      "supports_streaming": true
    },
    "vlm_fast": {
      "model_id": "Qwen/Qwen2-VL-2B-Instruct",
      "description": "快速场景判断，免费/低成本",
      "max_context_tokens": 8192,
      "supports_streaming": false,
      "use_case": "快速判断是否需要响应"
    },
    "vlm_detailed": {
      "model_id": "Qwen/Qwen2-VL-7B-Instruct",
      "description": "详细场景分析",
      "max_context_tokens": 32768,
      "supports_streaming": false,
      "use_case": "需要详细理解屏幕内容时使用"
    }
  },
  "conversation_trigger": {
    "fast_judge_model": "Qwen/Qwen2-VL-2B-Instruct",
    "screen_capture": {
      "low_frequency_interval_ms": 5000,
      "high_frequency_interval_ms": 1000,
      "max_recent_screens": 10
    },
    "proactive_rules": [
      {
        "name": "error_detection",
        "condition": "screen contains error message",
        "priority": 0.8
      },
      {
        "name": "idle_timeout",
        "condition": "user idle for 5 minutes",
        "priority": 0.5
      }
    ]
  }
}
```

### 17.10 集成到服务层主接口

```cpp
class AIService {
public:
    // ... 现有方法 ...
    
    // 多模态对话（完整流程）
    void processMultimodalConversation(
        const std::vector<uint8_t>& audioInput,
        std::function<void(const std::vector<uint8_t>&)> audioOutput
    ) {
        // 1. STT
        auto sttResult = m_speechService->speechToText(audioInput);
        
        // 2. 获取屏幕信息（低频，仅在需要时）
        std::optional<ScreenInfo> screenInfo;
        if (m_trigger->needsScreenContext(sttResult.text)) {
            screenInfo = m_visionService->captureScreen(false);
        }
        
        // 3. 判断是否需要响应
        auto decision = m_trigger->shouldRespondToInput(
            sttResult.text,
            screenInfo
        );
        
        if (decision.shouldRespond) {
            // 4. 生成响应
            auto response = chat(sttResult.text);
            
            // 5. TTS
            auto audio = m_speechService->textToSpeech(response.content);
            audioOutput(audio);
        }
    }
    
private:
    std::unique_ptr<SpeechService> m_speechService;
    std::unique_ptr<VisionService> m_visionService;
    std::unique_ptr<ConversationTrigger> m_trigger;
};
```

## 18. 总结

本服务层设计方案基于 SiliconFlow 官方文档设计，充分利用了硅基流动平台上的多种模型，通过智能路由和上下文管理，为不同类型的任务选择最合适的模型。设计特点：

1. **模型多样性**：支持多个模型，每个模型专注于擅长的任务类型
2. **智能路由**：基于任务类型、上下文大小、优先级等因素自动选择最佳模型
3. **高效并发**：请求队列、并发控制和优先级调度确保系统高效运行
4. **容错可靠**：完善的错误处理和重试机制
5. **易于扩展**：模块化设计，易于添加新模型和新功能
6. **API兼容性**：完全兼容 OpenAI API 格式，易于集成
7. **多模态支持**：集成 STT-VLM-TTS 能力，支持语音交互和视觉理解
8. **智能对话触发**：选择性响应机制，避免过度响应，支持主动对话
9. **资源优化**：屏幕信息低频采集策略，使用免费VLM快速判断场景

### 18.1 核心能力总结

#### 文本处理能力
- ✅ 代码生成、分析、审查
- ✅ 技术问答和对话
- ✅ Agent决策辅助
- ✅ 项目分析和架构设计

#### 多模态能力
- ✅ STT：语音转文本，支持实时流式识别
- ✅ TTS：文本转语音，支持多种音色和参数调节
- ✅ VLM：视觉语言模型，快速场景判断和详细屏幕理解
- ✅ 智能对话触发：选择性响应 + 主动对话

#### 工具调用能力
- ✅ Function Calling：支持 OpenAI 兼容的工具调用协议
- ✅ MCP Server：实现 Model Context Protocol，提供代码开发工具
- ✅ 代码工具集：read_file, list_files, search_code, get_project_structure, analyze_code
- ✅ 项目上下文收集：自动收集和分析项目信息
- ✅ 工具执行管理：安全的工具调用和执行

#### 系统特性
- ✅ 异步非阻塞处理
- ✅ 请求队列和并发控制
- ✅ 智能缓存机制
- ✅ 完善的错误处理和重试
- ✅ 灵活的配置管理

> **重要提醒**：实际部署时，请访问 SiliconFlow 官方文档和模型列表页面，确认：
> - 当前可用的模型ID
> - 各模型的实际上下文长度限制
> - 最新的价格信息
> - API的任何变更或限制

通过本方案的实施，Agent桌宠应用将能够高效、智能地利用AI能力，为用户提供卓越的体验。